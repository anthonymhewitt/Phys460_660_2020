{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Sampling --- Or, all stat mech is just computing integrals\n",
    "\n",
    "Let's say that you have some function $f$ defined over some domain. Let's also say that you want to compute the integral of $f$: \n",
    "\n",
    "$$\n",
    "I = \\int\\int f(x,y)dxdy\n",
    "$$\n",
    "\n",
    "How do you do this in the computer for any function $f$? Assume that you know the function $f$, but it cannot be integrated analytically. One solution is to *sample* the function $f$ at randomly chosen points in $x,y$. This is the essence of Monte Carlo simulation (and is also why it is called \"Monte Carlo,\" a place famous for casinos). The figure below shows how to compute the area of a circle by Monte Carlo simulation.\n",
    "\n",
    "<img src=\"./MonteCarloIntegrationCircle.svg\" alt=\"MC_circle\" width=\"200\"/>\n",
    "\n",
    "\n",
    "In this example we assume that we don't know how to compute the area of the circle, but we know where it's boundary is, because we know the function $f(x,y)$, and also that we know how to compute the area of the square. So we drop points at *randomly selected* locations inside the square, and we count how many of them land inside the circle. In the limit of many points, the fraction that land inside the circle multiplied by the area of the square gives you the area of the circle. Let's see how it works in code (but I will only compute the area of 1/4 of a circle).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "rand.seed()\n",
    "\n",
    "N=100\n",
    "\n",
    "x_in = []\n",
    "y_in = []\n",
    "x_out = []\n",
    "y_out = []\n",
    "N_in = 0\n",
    "\n",
    "for i in range(0,N):\n",
    "    # generate random points in a square of unit side length\n",
    "    x = rand.random()\n",
    "    y = rand.random()\n",
    "    # check whether the point falls inside an arc of radius 1\n",
    "    r_sqr = x**2 + y**2\n",
    "    if r_sqr < 1.0:\n",
    "        N_in = N_in + 1\n",
    "        x_in.append(x)\n",
    "        y_in.append(y)\n",
    "    else:\n",
    "        x_out.append(x)\n",
    "        y_out.append(y)\n",
    "\n",
    "#x_axis = np.linspace(0.0,50,50)\n",
    "#y_circ = np.sqrt(1.0 - x_axis**2)\n",
    "\n",
    "area = N_in/N\n",
    "area_calc = np.pi/4.0\n",
    "print(\"MC Area = \", area)\n",
    "print(\"pi/4 = \", area_calc)\n",
    "\n",
    "#plt.plot(x_axis,y_circ)\n",
    "plt.axis('equal')\n",
    "#plt.xlim(left=0.0)\n",
    "#plt.xlim(right=1.0)\n",
    "plt.scatter(x_in, y_in)\n",
    "plt.scatter(x_out, y_out)\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see a couple of things here:\n",
    "\n",
    "1. It actually works. Sometimes you will hear this sort of thing called \"numerical quadrature,\" a name which derives from the Pythagoreans. They determined areas by constructing squares with the same area as the shape, hence the name. Now it just means integrating.\n",
    "2. If we use more points, our estimate gets better. This is our first encounter with \"sampling errors,\" which arise from the finite-ness of sampling in a problem like this. If we were to generate many area estimates from $N=100$, they would scatter around the true area --- there is a *statistical uncertainty* associated with our estimate.\n",
    "3. We could also have a *systematic error.* What if we used a crappy random number generator, which tended to generate real numbers closer to 1 than 0? Is there any evidence for systematic error in our area calculation above?\n",
    "4. The MC method can be used to generate samples from *any* distribution. In effect, we have transformed an i.i.d. set of random numbers from 0 to 1 into a set that is contained within a circle...but we could use something besides `if (x^2 + y^2) < 1.0` to decide which ones to keep. Usually it is better to use an analytical transformation if one exists (this is how Python generates random numbers from a Gaussian, or weibull, etc), because we don't get to use any of the orange points. \n",
    "5. If we want to do stat mech in the computer, the problem is to generate samples from the Boltzmann distribution. This is why MC is useful for stat mech. \n",
    "\n",
    "**Question:** How should the statistical uncertainty decrease with $N$? (Note that the samples are all independent of one another. Here is another place your random number generator can cause problems!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations, Boltzmann, and Detailed Balance\n",
    "\n",
    "How do we map this onto an interesting stat mech problem? Recall that in stat mech and thermo we spend a lot of time talking about *microstates* and *configurations.* By that we mean the specification of the exact state of a thermodynamic system: What are the exact positions and locations of every atom in an ideal gas? In which direction is every single spin in a paramagnet pointing? Then, given a Hamiltonian, we can (try to) say something about the macrostate (thermodynamics: PV=NkT, e.g.) by summing over all the possible microstates. In case you forgot how this works, let's look at a specific case, and also recall the *Ising model.*\n",
    "\n",
    "## Configurations of the Ising model\n",
    "\n",
    "The Ising model is a simple stat mech model for magnetic materials. Imagine partitioning a material into small (nanoscopic) domains, and let each domain have associated with it a magnetic spin. This spin may arise from electronic degrees of freedom, or nuclear ones, or both. If all (or most) the spins point in the same direction the material obtains an overall magnetic moment. In the Ising model we only permit up and down orientations of the spins, and we arrange them on a lattice:\n",
    "\n",
    "<img src=\"./Ising.png\" alt=\"MC_circle\" width=\"200\"/>\n",
    "\n",
    "We can then specify the configuration of the system by listing the state of each of the $N$ lattice sites. If we let \"up\" be $1$ and \"down\" be $-1$ and denote the state of each site by $n_j$ , then the configuration in the image would be\n",
    "\n",
    "$$C = \\{-1,-1,-1,-1,-1,-1,-1,-1,-1,+1,-1,-1,...,n_j,...,+1,+1,+1,+1...\\}$$\n",
    "\n",
    "Are there some configurations that are more likely than others? If the system is *isolated,* the answer is No! This is the *assumption of equal a priori probabilities* on which all of stat mech is built. \n",
    "\n",
    "But if we ask instead \"How many microstates correspond to $N/2$ spins up?,\" the answer is \"a lot\" when N is large. However, if instead we ask \"how many states correspond are there with all spins up?,\" the answer is $1$. This difference is the microscopic basis of entropy --- the first case has a higher entropy, because there are more ways of doing it. \n",
    "\n",
    "**MC for an Ising model.** Now we are ready to map this back onto the area of the circle calculation above. However, instead of generating points in the plane at random, we will generate *microscopic configurations* at random. For this simple case, it is easy to do:\n",
    "\n",
    "    for i < totMCS\n",
    "        for j < N:\n",
    "            draw a rand\n",
    "            if rand < 0.5 set n_j = 1\n",
    "            else set n_j = -1\n",
    "\n",
    "Notice a couple things here:\n",
    "1. Each configuration requires N rands. This is a much higher dimensional problem than the circle problem! This is why such problems are called \"many-body.\" (OK, that usually implies interactions as well, but we will get to that.)\n",
    "2. There is an outer loop which sets the total number of samples (configurations) to be generated. Each time a new state is generated (on average) for all N lattice sites, we call it a Monte Carlo Sweep. \n",
    "\n",
    "Let's see if this can reproduce for us the entropy of the paramagnet. I will use the code above to generate configurations of a paramagnet in the absence of any external field, and I will keep a tally on how many times we find configurations with $N/2$ spins up, $N/2 - 1$ spins up, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "totMCS = 50000\n",
    "\n",
    "N_up = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,totMCS):\n",
    "    count = 0\n",
    "    for j in range(0,N-1):\n",
    "        tmp = rand.randrange(2)\n",
    "        if tmp == 0:\n",
    "            count = count+1\n",
    "        elif tmp == 1:\n",
    "            #do nothing\n",
    "            tmp2 = tmp\n",
    "        else:\n",
    "            print(\"rand \", tmp, \"out of range\")\n",
    "    \n",
    "    #tmp2 = np.log(count)\n",
    "    #N_up.append(tmp2)\n",
    "    N_up.append(count)\n",
    "    \n",
    "log_N = np.log(N_up)\n",
    "\n",
    "plt.hist(N_up,bins=53)\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Boltzmann factor\n",
    "\n",
    "Now we are starting to see that it takes a lot of sampling to get an estimate for a thermodynamic quantity like entropy. In this case, we do ok because *every single randomly generated configuration is \"good,\"* where by \"good\" we mean useful. None of them get thrown away, like in the example where we computed the area of a circle.\n",
    "\n",
    "But what if we are instead studying the paramagnetic in an external field, and in contact with a thermal bath at absolute temperature $T$? Now it is no longer the case that \"all configurations are created equal.\" If the field is strong relative to the thermal energy scale, then the spins want to line up to the field, on average. If we generate configurations randomly, then many of them will be not terribly useful. In order to be more precise, we need to review the stat mech treatment of a paramagnet in an external field.\n",
    "\n",
    "If we add an external field $\\bf{B}$ then the energy of one of our spins in that field (we let the magnetic moment of our spins be $\\mu$) is $-\\bf{\\mu\\cdot B}$. Since we only have two possible orientations we can only have $\\pm\\mu B$. Without loss of generality we can let the $B$ field point up, and therefore a single up spin has an energy of $-\\mu B$. The Hamiltonian for $N$ spins is therefore\n",
    "\n",
    "$$H(C) = -\\mu B \\sum_{i=1}^N \\sigma_i$$\n",
    "\n",
    "where we have introduced $\\sigma_i = \\pm 1$ to denote the state of the ith spin.\n",
    "\n",
    "Now it's time to recall the \"Boltzmann factor.\" If a system is in contact with a thermal reservoir at absolute temperature $T$, it is no longer the case that all configurations are equally likely. Instead, configurations of lower energy are more likely:\n",
    "\n",
    "$$P(C) \\propto e^{-H(C)/k_BT}$$\n",
    "\n",
    "where $k_B$ is Boltzmann's constant. The exponential factor is called the Boltzmann factor.\n",
    "\n",
    "**Normalization and the partition function:** We can change the $\\propto$ to an $=$ by demanding that $P({C})$ be normalized: \n",
    "\n",
    "$$1 = \\sum_{\\{C\\}}P(C) = \\frac{1}{Z}\\sum_{\\{C\\}}e^{-H(C)/k_BT}$$\n",
    "\n",
    "The last sum is called the *partition function,* is abbreviated $Z(T)$, and is the normalization of the probability distribution:\n",
    "\n",
    "$$P(C) = \\frac{e^{-H(C)/k_BT}}{Z(T)}$$\n",
    "\n",
    "So, now imagine you want to compute, say, the average energy $\\langle E\\rangle$. One way to do this would be to recognize that given a probability distribution, we get averages in the following way:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\langle E\\rangle & = \\sum_{\\{C\\}}P(C)H(C) \\\\\n",
    "& = \\sum_{\\{C\\}}H(C)\\frac{e^{-H(C)/k_BT}}{Z(T)}\n",
    "\\end{align}$$\n",
    "\n",
    "Therefore, if I generate a bunch of configurations completely at random as above, I add up the energies of each configuration, weighted by the Boltzmann factor.\n",
    "\n",
    "**Question:** What's wrong with this strategy? (Hint: Think about how many configs you would effectively \"throw out\" in this scheme.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis sampling and detailed balance: To flip, or not to flip\n",
    "\n",
    "Because of the problem just mentioned it is usually a bad idea to generate configurations completely at random. Instead, imagine that you already have a configuration that is \"pretty good\" in the sense that the energy is \"low,\" where low is set by $k_BT.$ Then let's generate a new configuration by picking a spin at random and trying to flip it. If that flip would lower the system energy (i.e., align the spin to the field) then we do it. But if it would increase the system energy a bit, then we might do it with some probability. After all, the spins can \"borrow\" a little thermal energy from the bath to do work against the field, and so these types of moves should become more likely at higher temperature. Repeat this over and over again and you will eventually evolve to a whole new configuration of the lattice, but always staying near the relevant (high Boltzmann weight) part of configuration space.\n",
    "\n",
    "But how do we set the probability to flip against the magnetic field? We can find a family of such rules by considering how the probability distribution changes in time as we flip spins. Let $W(C \\rightarrow C^{\\prime})$ be the (as yet undecided) rate at which we transition from configuration $C$ to $C^{\\prime}$. Then the probability will evolve in time according to a simple gain/loss differential equation:\n",
    "\n",
    "$$\\frac{\\partial P(C,t)}{\\partial t} = \\sum_{\\{C^{\\prime}\\}}[ P(C^{\\prime},t)W(C^{\\prime} \\rightarrow C) - P(C,t)W(C \\rightarrow C^{\\prime}) ]$$\n",
    "\n",
    "The first term is all of the configurations that flow *in* to configuration $C$ from any other configuration $C^{\\prime}$, the second term is all of the configurations that flow *out* from $C$ to any other configuration $C^{\\prime}$.\n",
    "\n",
    "Now if we are at equilibrium, the probability distribution doesn't depend on time, so the L.H.S. is zero. In this case, all of the transitions flowing into $C$ must be balanced by transitions out of $C$. So, we can choose our transition rates $W$ (to flip, or not to flip?) any way that we want so long as they satisfy that criterion.\n",
    "\n",
    "One way to do so is to demand that sum vanishes *term by term*: That is, for every pair of configs $C$ and $C^{\\prime}$, demand that\n",
    "\n",
    "$$ 0 = P(C^{\\prime})W(C^{\\prime} \\rightarrow C) - P(C)W(C \\rightarrow C^{\\prime})$$\n",
    "\n",
    "Rearranging, we obtain the *detailed balance* criterion:\n",
    "\n",
    "$$\\frac{P(C^{\\prime})}{P(C)} = \\frac{W(C \\rightarrow C^{\\prime})}{W(C^{\\prime} \\rightarrow C)}$$.\n",
    "\n",
    "Next, we subsitute in the equilibrium probability distribution:\n",
    "\n",
    "$$\\frac{e^{-H(C^{\\prime})/k_BT}/Z(T)}{e^{-H(C)/k_BT}/Z(T)} =  \\frac{W(C \\rightarrow C^{\\prime})}{W(C^{\\prime} \\rightarrow C)}$$\n",
    "\n",
    "**Notice:** the partition functions cancel. Rewriting the ratio of exponents, we finally arrive at the following expression:\n",
    "\n",
    "$$e^{[H(C)-H(C^{\\prime}]/k_BT} =  \\frac{W(C \\rightarrow C^{\\prime})}{W(C^{\\prime} \\rightarrow C)}$$\n",
    "\n",
    "This says that we can use any rule that we want, so long as the forward and back rates balance in this way. Notice also that when you are considering whether or not to flip a spin, you only need to compute the Boltzmann factor for the current and new configurations. \n",
    "\n",
    "## The Metropolis Rate\n",
    "\n",
    "In a 1953 paper, Metropolis, Rosenbluth, Rosenbluth, Teller and Teller[1] proposed the following rate $W(C \\rightarrow C^{\\prime})$:\n",
    "\n",
    "$$ W(C \\rightarrow C^{\\prime}) = \\begin{cases}\n",
    "  1 & \\text{if  } \\Delta H < 0 \\\\\n",
    "  e^{-\\Delta H/k_BT} & \\text{if  } \\Delta H > 0\n",
    "  \\end{cases}$$\n",
    "  \n",
    "where $\\Delta H = H(C) - H(C^{\\prime})$. In other words, if a spin flip lowers the overall energy, we let it happen. If it increases the overall energy, we let it happen with a probability $e^{-\\Delta H/k_BT}$. \n",
    "\n",
    "**Aside**: The MC rate now known as the \"Metropolis Rate\" was developed in 1953 at Los Alamos National Lab. The last Teller in the list is Edward M Teller, the \"father\" of the hydrogen bomb (no matter how bad your kids turn out to be, at least they aren't h-bombs), and one of the models for the character of Dr. Strangeglove (the other was Herman Kahn). The other Teller was his wife. The Rosenbluths were also a husband wife team...at that time at LANL and elsewhere, it was the women who were the computer programmers. The computer was the MANIAC, and the code was written on stacks of punchcards. The simulation methods back then were mostly for weapons development. If you want to read a sordid tale of scientific rancor, read the Wikipedia article on Metropolis et al.\n",
    "\n",
    "**The algorithm.** Here is how you implement the Metropolis rate for a MC simulation of something like an Ising model:\n",
    "\n",
    "    initialize the lattice in a random config of up and down spins\n",
    "    for i < totMCS\n",
    "        for j < N   ## N is the total number of spins\n",
    "            pick a spin at random\n",
    "            compute its energy E0\n",
    "            compute the energy E1 if it were to be flipped\n",
    "            if E1 < E0\n",
    "                flip it\n",
    "            if E1 > E0\n",
    "                compute P = exp[-(E1-E0)/kT]\n",
    "                roll a random number R uniformly distributed from [0,1)\n",
    "                if R < P\n",
    "                    flip the spin\n",
    "                else\n",
    "                    do nothing\n",
    "\n",
    "Let's note a few things about this algorithm:\n",
    "\n",
    "1. Instead of generating configurations uniformly and randomly and giving them a weight $e^{-H(C)/k_BT}$, we generate configurations with a probability $e^{-H(C)/k_BT}$, and weight them uniformly.\n",
    "2. That means that if want to compute something like the average energy $\\langle E \\rangle$, we simply run the algorithm for a whole bunch of Monte Carlo sweeps, and record the total energy occasionally, then compute its arithmetic mean. So if we take a total of $M$ samples of the energy: $\\langle E \\rangle = \\frac{1}{M}\\sum_{i=1}^M E_i$\n",
    "3. But how often should we sample the energy, or any other observable? Notice that the configurations that we generate in this manner are *correlated.* After we flip one spin, the configuration is almost the same as the last one. Even after we loop over all $N$ spins, it might still be very similar, depending on how \"easy\" it is to flip them.\n",
    "4. For this reason, we say that algorithms like MC generate a \"chain\" of configurations. In this case, the chain is \"Markovian,\" which means that each configuration only depends on the previous one. For that reason, we call this a \"Markov chain Monte Carlo\" (MCMC) algorithm.\n",
    "5. A good computational physicist will try to get a handle on such correlations and try to account for them when estimating error bars. More on this later.\n",
    "6. Notice that the initial config is random, and therefore doesn't look like a typical equilibrium config (except at infinite temperature). Therefore you have to discard some number of MC sweeps before recording data. There is no hard and fast rule for this --- it depends on the typical relaxation timescales in the system. \n",
    "7. Not all attempted \"moves\" are accepted. The ratio of successful to attempted moves is called the *acceptance ratio.* Clearly, there is some connection between the acceptance ratio and the efficiency of the algorithm (it should be neither too low nor too high), but again, there is no hard and fast rule for what it should be. What ultimately matters for the efficiency of a MC algorithm is the rate at which *uncorrelated* configurations (i.e., new informtation) are generated per unit *computer time.* \n",
    "\n",
    "Let's put the Metropolis algorithm to work on the Ising paramagnet in an external field. We will only have one parameter, which is $\\mu B/k_BT$, and we will generate a timeseries for the total energy, and its histogram.\n",
    "\n",
    "[1] \"Equation of State Calculations by Fast Computing Machines,\" Metropolis, et al. *J. Chem. Phys.* **21**:1087-1092(1953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a simple Metropolis MC algorithm for the paramagnet in an external field\n",
    "\n",
    "def print_lattice(N, spins):\n",
    "    for j in range(0,N):\n",
    "        if (j+1)%L != 0:\n",
    "            print(\"%3d\" % spins[j],end='')\n",
    "        else:\n",
    "            print(\"%3d\" % spins[j])\n",
    "    print()\n",
    "    print()\n",
    "    return;\n",
    "\n",
    "N = 16384\n",
    "L = 128\n",
    "write_freq = 10 # how frequently to write data\n",
    "discard = 100  # how many MC sweeps to discard before writing data\n",
    "totMCS = 5000\n",
    "muB = 1.0\n",
    "kT = 1.0\n",
    "Escale = muB/kT #this is uB/k_BT\n",
    "#deltaE = -2.0*muB\n",
    "Boltz_fact = np.exp(-2.0*Escale) # Boltzmann factor for flipping one spin\n",
    "print(\"Boltzm fact\", Boltz_fact)\n",
    "\n",
    "#this list will hold our lattice\n",
    "spins = []\n",
    "E = [] # this will store the energy of configs\n",
    "t = [] # this will store the \"time\" in MC sweeps\n",
    "N_up_list = []\n",
    "\n",
    "totN_up = 0\n",
    "##initialize the lattice\n",
    "for j in range(0,N):\n",
    "    #generate either -1 or +1\n",
    "    tmp = 2*rand.randrange(2) - 1\n",
    "    spins.append(tmp)\n",
    "    if tmp > 0:\n",
    "        totN_up = totN_up + 1\n",
    "\n",
    "#t.append(0)\n",
    "#Etmp = N - 2*totN_up\n",
    "#E.append(Etmp)\n",
    "\n",
    "\n",
    "\n",
    "## print lattice out at the beginning\n",
    "#print_lattice(N,spins)\n",
    "\n",
    "data_file = open(\"energy.dat\", \"w\")\n",
    "\n",
    "N_succ = 0 # we will use this to track the \"acceptance rate\" --- how often \n",
    "            # an attempted spin flip is successful. \n",
    "for i in range(1,totMCS):\n",
    "    \n",
    "    #track the total number of up spins for computing enery, magnetization, etc\n",
    "    totN_up = 0 \n",
    "    # the following loop performs one MCS, selecting \n",
    "    # sites to update at random\n",
    "    for j in range(0,N):\n",
    "        site = rand.randrange(N)\n",
    "        # pick a site a random\n",
    "        # here we check the current state of the selected spin. we \n",
    "        # let the be field be in the positive direction. therefore, \n",
    "        # if the spin is down, flipping it lowers the energy. that is the \n",
    "        # first if condition\n",
    "        if spins[site] < 0:\n",
    "            spins[site] = 1\n",
    "            totN_up = totN_up + 1\n",
    "            N_succ = N_succ + 1 # successful attempt\n",
    "        else: # it points up. so compute exp(-deltaE) and check against a rand\n",
    "              # notice that we already know deltaE (it is 2*Escale), so we already computed\n",
    "              # exp(-deltaE) outside the loop\n",
    "            tmp = rand.random()\n",
    "            if tmp < Boltz_fact:\n",
    "                spins[site] = -1 # flip the spin\n",
    "                N_succ = N_succ + 1 # successful attempt\n",
    "            else:\n",
    "                totN_up = totN_up + 1 # spin stays aligned to the field\n",
    "    #print(totN_up)\n",
    "    \n",
    "    #check whether it is time to write some data\n",
    "    if i > discard and i%write_freq == 0:\n",
    "        #totN_up = 0\n",
    "        #for j in range(0,N):\n",
    "        #    if spins[j] > 0:\n",
    "        #        totN_up = totN_up + 1\n",
    "        t.append(i)\n",
    "        Etmp = N - 2*totN_up\n",
    "        E.append(Etmp)\n",
    "        data_file.write(\"%d \\n\" % Etmp)\n",
    "        \n",
    "        \n",
    "data_file.close()\n",
    "\n",
    "#compute ratio of successful flips into total attempts: the success rate\n",
    "acc_rate = N_succ/(N*totMCS)  \n",
    "print(\"MC Acceptance rate: \",acc_rate)\n",
    "\n",
    "## print the lattice at the end\n",
    "#print_lattice(N,spins)\n",
    "\n",
    "plt.figure(1, figsize=(8.5, 4))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "ax = plt.subplot(121)\n",
    "ax.set_xlabel('MC sweeps')\n",
    "ax.set_ylabel('Energy')\n",
    "ax.set_xlim(left=10000)\n",
    "ax.set_xlim(right=10500)\n",
    "plt.plot(t,E)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.set_xlabel('Energy')\n",
    "ax2.set_ylabel('Histo counts')\n",
    "plt.hist(E,bins=40)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Paramagnet physics\n",
    "\n",
    "Above, we noted the importance of the partition function, $Z(\\beta)$, where $\\beta = 1/k_BT$. Recall that its central role in stat mech derives from the fact that you can obtain quantitaties like the average energy, the magnetization, or the heat capacity by taking appropriate derivatives of $\\text{ln}Z$. For example, we obtain the average energy $\\langle E \\rangle$ by a derivative w.r.t $\\beta$:\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\frac{\\partial \\text{ln}(Z)}{\\partial \\beta}  & = -\\frac{1}{Z}\\frac{\\partial}{\\partial \\beta} \\sum_{\\{C\\}}e^{-\\beta H(C)} \\\\\n",
    "& = -\\frac{1}{Z}\\sum_{\\{C\\}}H(C) e^{-\\beta H(C)} \\\\\n",
    "& = \\langle E \\rangle\n",
    "\\end{align}$$\n",
    "\n",
    "Now, normally computing the partition sum exactly is not possible (hence, MC methods), but in the case of the paramagnet it is. Let's recall how to do it. Substituting in the paramagnet Hamiltonian from above $H(C) = -\\mu B \\sum_{i=1}^N \\sigma_i$:\n",
    "\n",
    "$$\\begin{align}\n",
    "Z(\\beta) & = \\sum_{\\{C\\}}\\left[\\text{exp}\\left(\\beta \\mu B  \\sum_{i=1}^N \\sigma_i\\right)\\right] \\\\\n",
    "& = \\prod_{j=1}^N\\left[ \\text{exp}\\left(\\beta\\mu B\\sum_{\\sigma_j = \\pm 1}\\sigma_j\\right)\\right] \\\\\n",
    "& = \\prod_{j=1}^N\\left[ e^{\\beta\\mu B} + e^{-\\beta\\mu B}  \\right] \\\\\n",
    "& = \\left[ e^{\\beta\\mu B} + e^{-\\beta\\mu B}  \\right]^N\n",
    "\\end{align}$$\n",
    "\n",
    "We can readily compute the derivative that we need to obtain the average energy of the paramagnet: \n",
    "$$\\begin{align}\n",
    "\\langle E \\rangle & = -\\frac{\\partial}{\\partial \\beta} \\text{ln}\\left[ e^{\\beta\\mu B} + e^{-\\beta\\mu B}  \\right]^N \\\\\n",
    "& = -N \\frac {\\partial}{\\partial \\beta} \\text{ln}\\left[ e^{\\beta\\mu B} + e^{-\\beta\\mu B}  \\right] \\\\\n",
    "& = -N\\mu B\\left[\\frac{e^{\\beta\\mu B} - e^{-\\beta\\mu B}}{e^{\\beta\\mu B} + e^{-\\beta\\mu B}}  \\right] \\\\\n",
    "& = -N\\mu B \\text{tanh}(\\beta\\mu B)\n",
    "\\end{align}$$\n",
    "\n",
    "We note a couple of things:\n",
    "1. The average energy increases with $N$. This makes sense --- energy is an extensive quantity, after all.\n",
    "2. The argument of the tanh is $\\mu B/k_BT$. This means that the high temperature limit is defined by $\\mu B << k_BT$. In this limit, we expect to see roughly equal numbers of up spins as down (entropy dominates).\n",
    "3. The low T limit is defined by $\\mu B >> k_BT$. In this regime, the magnetic field tends to align the spins, and the average energy approaches $-N\\mu B$. \n",
    "\n",
    "**Question:** What about the regime in which $\\langle E \\rangle > 0$? Where is it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluctuations, heat capacity, and response functions \n",
    "\n",
    "Here we will derive an important relationship between fluctuations of the energy in the canonical ensemble and the heat capacity at constant volume. Recall that the heat capacity is \n",
    "\n",
    "$$C_V(T) = \\frac{\\partial \\langle E \\rangle}{\\partial T}$$\n",
    "\n",
    "If we take the derivative of our expression just derived for $\\langle E \\rangle$:\n",
    "$$\\begin{align}\n",
    "C_V & = \\frac{\\partial \\langle E \\rangle}{\\partial T} \\\\\n",
    "& = \\frac{\\partial}{\\partial T}\\left(\\frac{\\partial \\text{ln}Z(\\beta)}{\\partial \\beta} \\right) \\\\\n",
    "& = -\\frac{1}{k_BT^2}\\left[-\\frac{1}{Z^2}\\frac{\\partial^2Z}{\\partial \\beta^2} + \\frac{1}{Z^2}\\left(\\frac{\\partial Z}{\\partial\\beta}\\right)^2  \\right] \\\\\n",
    "& = -\\frac{1}{k_BT^2}\\left(\\langle E^2 \\rangle - \\langle E \\rangle^2 \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "So: The heat capacity is proportional to the fluctuations in the energy. This is an instance of a \"fluctuation-response\" theorem, which relates fluctuations in a quantity at equilibrium to the response of the system, in this case to a change in temperature. (There is a similar relation for the magnetization and the magnetic susceptibility, which you will explore in project 3.) Later, we will see that fluctuations (and therefore the heat capacity) diverge at a *critical point,* where a system passes through a continuous phase transition.\n",
    "\n",
    "**The energy is Gaussian-distributed in the canonical ensemble.** Above, we derived a relationship between the fluctuations in the energy and the heat capacity. Here, we will consider more closely how the energy distributes itself around its mean value in the canonical ensemble. Consider sampling many values of the energy in a MC simulation, labelled $E_i$, and then consider the quantity\n",
    "\n",
    "$$E_i - \\langle E \\rangle$$\n",
    "\n",
    "This tells you how the energy is distributed around its mean. But how likely is a fluctuation of a particular size? [fig here] For this we need to consider some kind of *average deviation.* Consider\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sigma_E^2 & \\equiv \\langle \\left( E - \\langle E \\rangle \\right) ^2 \\rangle \\\\\n",
    "& = \\langle E^2 \\rangle - \\langle E \\rangle ^2\n",
    "\\end{align}$$\n",
    "\n",
    "The reason why I called this quantity $\\sigma_E^2$ is because it *is* the variance of the distribution of the energy. The second line shows us that it is related to the heat capacity via the fluctuation-response theorem. \n",
    "\n",
    "Moreover, if we imagine our samples of the energy $E_i$ to be taken from an infinitely long Monte Carlo simulation, with infinitely many MC sweeps in between them, then they must be *independent* of one another. If we take independent samples from a \"well-behaved\" distribution (i.e., one with a finite variance), then the central limit theorem tells us that those deviations from the mean must be Gaussian distributed. In other words:\n",
    "\n",
    "$$P(E) = \\text{exp}\\left( \\frac{(E-\\langle E \\rangle)^2}{2\\sigma_E^2}  \\right)$$\n",
    "\n",
    "So, now we know a couple more things about our energy histogram that we plotted above:\n",
    "1. It is a Gaussian (well, approximately Gaussian for big enough $N$). \n",
    "2. The variance of the distribution of the energy is directly proportional to the heat capacity. \n",
    "3. Note the assumption of \"well-behaved\" above. This assumption is violated at a critical point, where the system undergoes a second order phase transition. At such a point, fluctuations in the energy are observed on *all scales,* and therefore the heat capacity diverges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite sampling errors and error bars\n",
    "\n",
    "In motivating the use of the central limit theorem, we assumed an infinitely long MC simulation. This is of course never the case in practice, but how long is long enough? To answer this, we have to think more pragmatically about the goals of molecular simulation. (This applies to all simulations, not just MC.)\n",
    "\n",
    "We *don't* do simulations to make nice pictures (although they can help make a point, and get your work featured on journal covers). Rather, we want to *measure* something in the computer --- the average energy, or the magnetization, or the heat capacity. If you run a long MC simulation, compute the average energy, and report that number, you are not finished! This is because if you run the simulation again, you will get a different value for the average energy --- there is an error bar, that arises from finite sampling. If you gave me $M$ *independent* measurements (or estimates) of $\\langle E \\rangle$, then it is easy to compute the error bar: Just take the standard deviation of the estimates, and divide by the square root of $M-1$. This is called the \"standard error on the mean.\" It's a little confusing, so let me go through it step by step:\n",
    "1. Run a MC simulation and compute the average energy. Note that this is an *estimate* of $\\langle E \\rangle$. For that reason I will call it $\\bar{E}_i$. I give it an index since we will do this over and over.\n",
    "2. Run M simulations like this, and for each one compute $\\bar{E}_i$.\n",
    "3. Average $\\bar{E}_i$ to get your best estimate of $\\langle E \\rangle$ from the available data: $\\bar{E} = (\\sum_{i=1}^M \\bar{E}_i)/M$\n",
    "4. But how good is that estimate? What error bar do you report along with it? This is provided by the standard error:\n",
    "\n",
    "$$\\text{SE} = \\frac{\\sqrt{\\sum_{i=1}^M (\\bar{E}_i - \\bar{E})^2}}{\\sqrt{(M-1)}}$$\n",
    "\n",
    "*Remember:* We have said several times in the preceding discussion that the samples $E_i$ are *independent.*\n",
    "\n",
    "*Always* think about whether it makes sense to report an error bar with a number computing from a simulation! (It usually does. Or if you don't, you had better explain why.)\n",
    "\n",
    "**Sampling errors from a single simulation.** We started this discussion by asking \"how long is long enough?\" Then we discussed finite sampling errors, using multiple MC simulations to make the point. But now we return to the \"how long\" question, pointing out that over the course of a long MC simulation we should obtain independent samples, provided that we don't sample too frequently. How do we decide how many MC steps to discard between samples?\n",
    "\n",
    "The answer is provided by the correlation time --- after all, independent means uncorrelated. The procedure is therefore to measure the autocorrelation function, and determine the characteristic time $\\tau$ over which \"memory\" of previous configurations persists. If you sample configurations no more frequently than $tau$, you can reasonably expect them to be independent, and treat them as such in your error analysis. Below, we will show this for our paramagnet, but the results are boring. They are more interesting for the ferromagnet that you will study in the project 3.\n",
    "\n",
    "**Aside:** There is a more clever way to do this, reported in a J. Chem. Phys. article by Flyvbjerg and Petersen in 1989 (although they note that it is not their idea, they don't know where it came from, but they are reporting it for posterity). The basic idea is that if you compute the error from data that are sampled too frequently, you will underestimate errors --- variance will be lower, and $M$ is too big. However, in the regime where samples are independent, the error estimate is invariant under subsampling transformations. You should read this article if you work in computer simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#open the energy data from the MC sim\n",
    "#rh = open(\"energy.dat\", \"r\")\n",
    "\n",
    "Edat = np.loadtxt(\"energy.dat\")\n",
    "\n",
    "#MCtime = []\n",
    "\n",
    "#count = -1\n",
    "#for line in rh:\n",
    "#    count = count + 1\n",
    "#    Edat.append(line)\n",
    "#    MCtime.append(count)\n",
    "\n",
    "max_tau = 20\n",
    "totalT = len(Edat)\n",
    "#print(totalT)\n",
    "\n",
    "C_t = []\n",
    "\n",
    "total = 0\n",
    "for i in range(0,totalT):\n",
    "    total = total + Edat[i]\n",
    "\n",
    "avgE = total/totalT\n",
    "\n",
    "for tau in range(0,max_tau):\n",
    "    total = 0\n",
    "\n",
    "    M = 0\n",
    "    for j in range(0,totalT-max_tau):\n",
    "        M = M+1\n",
    "        total = total + Edat[j]*Edat[j+tau]\n",
    "    \n",
    "    \n",
    "    avg = total/M - avgE**2\n",
    "    C_t.append(avg)\n",
    "\n",
    "M = len(C_t)\n",
    "#print(M)\n",
    "var = C_t[0]\n",
    "for i in range(0,M):\n",
    "    #print(C_t[i])\n",
    "    C_t[i] = C_t[i]/var\n",
    "    #print(C_t[i])\n",
    "\n",
    "#M = len(C_t)\n",
    "#print(M)\n",
    "t_axis = np.linspace(0,M,M)\n",
    "\n",
    "plt.plot(t_axis,C_t)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase transitions and the ferromagnet\n",
    "\n",
    "The basic concept of a phase transition is familar from everyday life --- think of water freezing and melting, or boiling and condensing. We have the idea that there is somehow involved a transition from a more ordered state to a less ordered thermodynamic state, or vice versa. (There are also transitions between states of different crystal symmetry, but we will ignore them here.)\n",
    "\n",
    "How is a phase transition identified? One approach is to consider an *order parameter* which discriminates between the two phases. The order parameter for the liquid-vapor transition is the density --- the more dense phase is the liquid, the less dense phase the vapor. Recall from your thermo/stat mech that the boundary between the liquid and the vapor ends at a *critical point.* If you cross the boundary below the critical point, the density changes discontinuously. If you go right through the critical point, the density changes continuously as you go from vapor to fluid. The former is called a \"first order\" or dicontinuous phase transition, the latter a \"2nd order\" or \"continuous\" phase transition. At a first order phase transition, the derivative of free energy is discontinuous, while at a continuous transition it is continuous.\n",
    "\n",
    "Whether there exists a continuous transition (critical point, or line of critical points) between two phases is fundamentally related to the symmetry of the two phases. They must have the same symmetry in order for a critical point to exist (otherwise there is no way to continuously go from one to the other). \n",
    "\n",
    "The simple paramagnet that we have been studying so far is boring in this regard. Even though it is more or less ordered as $\\mu B/k_BT$ is varied, there is no phase transition that separates these regions. For that, we need to consider the ferromagnetic Ising model. The Hamiltonian is\n",
    "\n",
    "$$H(C) = -\\mu B \\sum_{i=i}^N\\sigma_i - J\\sum_{\\langle i,j \\rangle} \\sigma_i\\sigma_j$$\n",
    "\n",
    "The notation $\\langle i,j \\rangle$ means \"sum over nearest neighbors only.\" The ferromagnetic Ising model thus includes a coupling that tends to locally align the spins. Notice that the dimension now matters: On a one dimensional lattice of spins, each spin only has two neighbors, in 2D it has 4, and in 3D it has 6. \n",
    "\n",
    "The order parameter for the Ising model is the magnetisation per spin $m$:\n",
    "\n",
    "$$m = \\frac{1}{N}\\mu\\sum_{i=i}^N\\sigma_i $$\n",
    "\n",
    "which is positive when the spins are aligned to the field, and negative when they are on average antiparallel. In project 3, you will study the phase transitions in the ferromagnetic Ising model for $B = 0$ and $B \\neq 0$. When $B \\neq 0$, the system has a first-order transition --- as a function of temperature, there is a discontinuous jump in the magnetization as temperature is increased. At $B=0$, there is a critical point which occurs at a critical temperature $T_c = 2.269$. Above this temperature the system is disordered. Below $T_c$ the nearest neighbor interactions favor alignment of the spins. Since this happens in the absence of a magnetic field there is no difference anymore between up and down, and the system randomly selects a direction. This phenomenon is therefore called \"spontaneous symmetry breaking,\" since the up/down symmetry is broken in the absence of something (like a magnetic field) to pick a direction.\n",
    "\n",
    "**1st order transitions: Hysteresis** A system that has a 1st order transition will display *hysteresis.* This means that the transition occurs at different values of the control parameter ($T$ or $B$) in each direction (increasing or decreasing the control parameter). An example is shown in the figure:<img src=\"./hysteresis.jpg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "At a first order transition, the low temperature phase appears first as \"droplets\" of a characteristic size --- we say that it is nucleated. Forming such a droplet requires paying the cost of the boundary (think of surface tension for a liquid droplet in vapor), and so you have to \"overshoot\" the phase boundary in order to form the phase. However, the exact way in which this happens is history-dependent, hence the name \"hysteresis.\"\n",
    "\n",
    "**2nd order transitions: Scaling, critical exponents, and critical slowing down.** Second-order transitions are way more interesting. In contrast to 1st order transistions, at which a characteristic scale (droplet size) defines the length over which correlated fluctuations occur, at a 2nd order transition this lengthscale *diverges.* This behavior is captured by considering the correlation length $xi$, which measures the lengthscale over which a pair of spins are correlated:\n",
    "\n",
    "$$\\xi \\propto \\left| T - T_c\\right|^{-\\nu}$$\n",
    "\n",
    "For the 2D Ising model, $\\nu=1$ The magnetization also diverges at the critical point. As the system is cooled through the critical temperature $T_c$ from below, the magnetization scales like:\n",
    "\n",
    "$$M \\propto \\left(T_c-T\\right)^{\\beta}$$\n",
    "\n",
    "with $\\beta = 1/8$, obtained analytically by Onsager in the 50's. The response functions (heat capacity and magnetic susceptibility) also also diverge at the critical point, with their own exponents. \n",
    "\n",
    "One of the most remarkable discoveries in modern physics is the existence of *universality.* The exponents that describe the divergences at the critical point depend only on dimension, symmetry group of the Hamiltonian, and the range of the interaction (short or long). *Thus the Ising model (in 3D) and the van der Waal's fluid have the same exponents.* \n",
    "\n",
    "**Finite size and the thermodynamic limit.** In a finite system, nothing diverges, and simulations are always (very) finite. As you approach a critical point from above, the correlation length grows, but at some point it hits the edge of the simulatied lattice. I cannot grow without bound, and all divergences are \"rounded off.\" There is an extensive literature from the 80's (espcially David Landau, Bob Swendsen, Michael Fisher), but as a simple practical check one may simply compute response functions for systems of increasing size. \n",
    "\n",
    "*Aside: Periodic Boundary Conditions.* Speaking of which...what do we do about edge effects? By far the most common solution is to use periodic boundary conditions: The neighbors to the right of a spin on the far right edge are the first column on the left, etc. Note that this is still a finite system --- if the lattice is of size $L$, you cannot observe fluctuations with lengthscales longer than $L$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
